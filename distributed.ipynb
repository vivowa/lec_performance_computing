{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python distributed computing\n",
    "\n",
    "<br/>\n",
    "<div align=\"center\">18th of June, 2021</div>\n",
    "<br/>\n",
    "<div align=\"center\">\n",
    "    Thomas Arildsen<br/>\n",
    "    <a href=\"mailto:tari@its.aau.dk\">tari@its.aau.dk</a>\n",
    "<div/>\n",
    "<br/>\n",
    "<div align=\"center\">\n",
    "CLAAUDIA<br/>\n",
    "Aalborg University\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python distributed computing\n",
    "\n",
    "## Agenda\n",
    "\n",
    "* Using the `distributed` module\n",
    "* Integrating with `dask`\n",
    "* Alternative: IPython parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We have seen some theory and practice of parallel computing on *one* physical computer this morning.\n",
    "- Now we turn to distributed computing. That is, computing across two or more physical computers (each of which can have several CPUs/cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### *Symmetric Multi-Processing (SMP)* vs. distributed processing\n",
    "\n",
    "![Symmetric Multi-Processing](figures/SMP_-_Symmetric_Multiprocessor_System.svg)\n",
    "*Image source: [WikiPedia](https://commons.wikimedia.org/wiki/File:SMP_-_Symmetric_Multiprocessor_System.svg) (CC-BY-SA)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Symmetric Multi-Processing (SMP) vs. *distributed processing*\n",
    "\n",
    "![Symmetric Multi-Processing](figures/Beowulf.png)\n",
    "*Image source: [WikiPedia](https://commons.wikimedia.org/wiki/File:Beowulf.png) (public domain)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can we do distributed computing in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `distributed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Set up cluster\n",
    "\n",
    "We are going to cheat a bit and set up a scheduler and workers on our own computer and pretend these are running on separate computers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from distributed import LocalCluster\n",
    "lc = LocalCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The scheduler and workers can also be started from the command line - see: http://distributed.readthedocs.io/en/latest/setup.html#using-the-command-line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using the cluster - Dask\n",
    "\n",
    "In order to distribute tasks on the workers, we create a `dask.distributed.Client`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:51118</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>8.59 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:51118' processes=4 threads=8, memory=8.59 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(lc.scheduler_address)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now submit work on the cluster in the same way as we do when using a pool of workers in `concurrent.futures`.\n",
    "\n",
    "**SPMD-style processing**\n",
    "- We re-use an example from earlier today from `concurrent.futures` - now adapted to `dask.distributed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent process id:    2764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2776, 2778, 2777]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, time\n",
    "\n",
    "def _f(d):\n",
    "    # Defines the f(d) function (f(d) is a single task)\n",
    "    time.sleep(float(d)/10.)\n",
    "    pid = os.getpid()\n",
    "    print(\" _f argument: {:2d}, process id: {:7d} \".format(d, pid))\n",
    "    return pid\n",
    "\n",
    "print(\"Parent process id: {:7d}\".format(os.getpid()))\n",
    "future = client.map(_f, (30 ,15 ,2))\n",
    "client.gather(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**MPMD-style processing**\n",
    "- Again, we re-use an example from earlier today from `concurrent.futures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def _f1(d):\n",
    "    # Defines the f1(d) function (f(d) is a single task)\n",
    "    return d + 1\n",
    "\n",
    "def _f2(d):\n",
    "    # Defines the f1(d) function (f(d) is a single task)\n",
    "    return d + 2\n",
    "\n",
    "def _f3(d):\n",
    "    # Defines the f1(d) function (f(d) is a single task)\n",
    "    return d + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': # We have to use this to make it work in this interactive interpreter\n",
    "    futurelist = []\n",
    "    futurelist.append(client.submit(_f1, 1))\n",
    "    futurelist.append(client.submit(_f2, 1))\n",
    "    futurelist.append(client.submit(_f3, 1))\n",
    "    #print([future.result() for future in futurelist])\n",
    "    print(client.gather(futurelist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Notice how we could use `client.gather` on a list of result futures to get them all.\n",
    "- We could also call `.result()` on each of the individual result futures to retrieve them individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Caveats on distribution of data\n",
    "\n",
    "- When we submit tasks to the workers, the functions and data get serialised (pickled) and transmitted to the workers.\n",
    "- This is relatively inexpensive in the cases we saw earlier today where all processing takes place on one computer; objects and functions get copied in local memory.\n",
    "- In the case of distributed computing, the serialised data is transmitted across a network to the workers $\\rightarrow$ generally much slower.\n",
    "- Generally, the tasks we want to perform on the workers should take (much) longer than it takes to transmit the data to the workers. Otherwise, distributing the work is not worthwhile.\n",
    "- For large amounts of data, it is often better to keep this stored in files in a network location accessible to all the workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load data from file on workers\n",
    "\n",
    "In this example we demonstrate how the workers can load the data they need from files they can access locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ls datafile*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('*.hdf5')\n",
    "for idx, file in enumerate(files):\n",
    "    files[idx] = os.path.abspath(file)\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "absolute path in order to mitigate issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Define a function to load a file on a worker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import dask.array as da\n",
    "\n",
    "def load_file(filename):\n",
    "    f = h5py.File(filename,'r')\n",
    "    return f['/data'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "def load_file2(filename):\n",
    "    return da.random.normal(size=(100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let us try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "load_file(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can map the `load_file` function to the workers to let them load each their own data set from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "futures = client.map(load_file, files)\n",
    "futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bring the function to the data\n",
    "\n",
    "When you need to do further processing on a result that has been computed on the workers, it is usually much more efficient to keep the data on the workers and push the function you need computed to them instead of bringing the data back to the client computer.\n",
    "\n",
    "Slow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "futures[0].result().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The above fetches the $100 \\times 100$ NumPy array (large) that was loaded from file on the worker and queries its shape on the client computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "client.submit(lambda a: a.shape, futures[0]).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The above sends a function to the worker containing the data loaded in `futures[0]`. The function queries the shape property of the array. The resulting (small) shape tuple is then sent back to the client computer.\n",
    "- Note how this example uses a lambda function to call the shape property of the array since the shape property is not a function we can call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Integration with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use Dask arrays (which we saw earlier today) directly on top of `distributed`'s futures and thereby transparently use Dask in a distributed way.\n",
    "\n",
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "concat_future = client.submit(da.concatenate, futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = concat_future.result()\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`array` is now one big Dask array which we can manipulate on the client computer.\n",
    "- The data is not on the client computer; it sits on the worker nodes - distributed across them.\n",
    "- Dask now transparently takes care of the subsequent operations we do on the combined array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "client.compute(array[:10,:10]).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We just used Dask from the `distributed` side where we had already retrieved data on the workers and then put a Dask interface on top of it afterwards.\n",
    "\n",
    "We can also start from Dask if we like and set that up to use `distributed` \"under the hood\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y = da.arange(100, chunks=(25,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "client.compute(y).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Considerations about network and cluster set-up\n",
    "\n",
    "- For the sake of security, it is recommended to use a distributed set-up as we have sketched here on a secure network.\n",
    "- That is, you will typically run a set-up like this on a dedicated cluster where you have control over who can access the scheduler and worker nodes and the network between them is closed to outsiders.\n",
    "- This is to avoid the risk of unauthorised users interacting with the scheduler and submitting malevolent tasks etc.\n",
    "- So, a distributed worker set-up like this makes sense if you have access to a cluster where you can control the entire cluster or at least trust other users on the cluster.\n",
    "- If you do not have access to a cluster, `distributed` also lets you run distributed work on for example Amazon EC2: https://distributed.readthedocs.io/en/latest/ec2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IPython parallel\n",
    "\n",
    "IPython also includes functionality to run a cluster for distributed computing.\n",
    "- The security aspects mentioned before are particularly important for IPython parallel: http://minrk-parallel.readthedocs.io/en/latest/security.html\n",
    "\n",
    "Using IPython parallel is similar to `distributed`. We illustrate it with a small example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Install `ipyparallel`\n",
    "\n",
    "```bash\n",
    "conda install ipyparallel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Set up cluster\n",
    "\n",
    "As before, we are going to cheat and set up a scheduler and workers on our own computer and pretend these are running on separate computers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Start both scheduler and 4 workers using the following command:\n",
    "```bash\n",
    "ipcluster start -n 4\n",
    "```\n",
    "- I will demonstrate alongside so you can see what it should look like.\n",
    "- The above starts a controller and 4 engines on the local computer. To start controller and engines on different computers in a cluster see further set-up details here: http://minrk-parallel.readthedocs.io/en/latest/process.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using the cluster\n",
    "\n",
    "In order to distribute tasks on the engines, we need to import `ipyparallel` on the client computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "\n",
    "c = ipp.Client()\n",
    "\n",
    "c.ids\n",
    "\n",
    "c[:].apply_sync(lambda : \"Hello, World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "One way to distribute tasks to the workers in a similar way to previous examples we have used, we use `ipyparallel.LoadBalancedView`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**SPMD-style processing**\n",
    "- We re-use an example from earlier today from `concurrent.futures` - now adapted to `ipyparallel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "@ipp.require('os', 'time')\n",
    "def _f(d):\n",
    "    # Defines the f(d) function (f(d) is a single task)\n",
    "    time.sleep(float(d)/10.)\n",
    "    pid = os.getpid()\n",
    "    print(\" _f argument: {:2d}, process id: {:7d} \".format(d, pid))\n",
    "    return pid\n",
    "\n",
    "print(\"Parent process id: {:7d}\".format(os.getpid()))\n",
    "lview = c.load_balanced_view()\n",
    "result = lview.map_async(_f, (30 ,15 ,2))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "result.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Note the `@ipp.require` decorator to ensure that necessary modules are available on the workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**MPMD-style processing**\n",
    "- Again, we re-use an example from earlier today from `concurrent.futures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def _f1(d):\n",
    "    # Defines the f1(d) function (f(d) is a single task)\n",
    "    return d + 1\n",
    "\n",
    "def _f2(d):\n",
    "    # Defines the f1(d) function (f(d) is a single task)\n",
    "    return d + 2\n",
    "\n",
    "def _f3(d):\n",
    "    # Defines the f1(d) function (f(d) is a single task)\n",
    "    return d + 3\n",
    "\n",
    "futurelist = []\n",
    "futurelist.append(lview.apply_async(_f1, 1))\n",
    "futurelist.append(lview.apply_async(_f2, 1))\n",
    "futurelist.append(lview.apply_async(_f3, 1))\n",
    "print([future.result() for future in futurelist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Integration with Dask\n",
    "\n",
    "IPython Parallel can also provide a cluster for Dask\n",
    "\n",
    "* Similar to how we saw it done with `dask.distributed`\n",
    "* See: https://distributed.readthedocs.io/en/latest/ipython.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
