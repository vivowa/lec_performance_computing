{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python parallel computing\n",
    "\n",
    "<br/>\n",
    "<div align=\"center\">18th of June, 2021</div>\n",
    "<br/>\n",
    "<div align=\"center\">\n",
    "    Thomas Arildsen<br/>\n",
    "    <a href=\"mailto:tari@its.aau.dk\">tari@its.aau.dk</a>\n",
    "<div/>\n",
    "<br/>\n",
    "<div align=\"center\">\n",
    "Dept. of Electronic Systems<br/>\n",
    "Aalborg University\n",
    "</div>\n",
    "\n",
    "*Partly based on material by Torben Larsen, T. Arildsen and T. L. Jensen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python parallel computing\n",
    "\n",
    "## Agenda\n",
    "\n",
    "* Using the `multiprocessing` module\n",
    "* Using the `concurrent.futures` module\n",
    "* Somewhat different: `dask`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We have looked a bit at theoretical details on parallel computing.\n",
    "- We first focus on parallel computing across *one* physical computer (i.e. multiple CPUs / CPU cores with shared memory)\n",
    "- Now, what are the possibilities in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Processes vs threads\n",
    "\n",
    "**Process**\n",
    "\n",
    "*In computing, a process is an instance of a computer program that is being executed. It contains the program code and its current activity. Depending on the operating system (OS), a process may be made up of multiple threads of execution that execute instructions concurrently.* - [WikiPedia](https://en.wikipedia.org/wiki/Process_%28computing%29)\n",
    "\n",
    "**Thread**\n",
    "\n",
    "*A thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler, which is typically a part of the operating system. The implementation of threads and processes differs between operating systems, but in most cases a thread is a component of a process. Multiple threads can exist within one process, executing concurrently and sharing resources such as memory, while different processes do not share these resources. In particular, the threads of a process share its executable code and the values of its variables at any given time.* - [WikiPedia](https://en.wikipedia.org/wiki/Thread_%28computing%29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Threads in Python\n",
    "\n",
    "- Python can have threads (see the `threading` module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Execution in Python is limited to one thread at a time due to the *Global Interpreter Lock* (GIL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This means that threads cannot execute in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Still, threads in Python can be a nice tool to improve performance of I/O-heavy operations, e.g. [this example](https://www.toptal.com/python/beginners-guide-to-concurrency-and-parallelism-in-python)\n",
    "\n",
    "- However, we focus on parallel computing here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `multiprocessing`\n",
    "\n",
    "- Part of Python's standard library, a \"classic\" workhorse in parallel computing in Python\n",
    "\n",
    "A few things to be aware of with `multiprocessing` (i.e. due to processes - not threads):\n",
    "\n",
    "- ‘Heavy’ parallelism $\\rightarrow$ process parallel meaning that we essentially make a number of Python interpreters running in parallel, each with their own full copy of whatever needed.\n",
    "- This is administratively ‘heavy’ $\\rightarrow$ copies of environments, variables, arrays, imports etc.\n",
    "- Heavy on memory as we copy the full workspace for each parallel entity.\n",
    "- But fortunately we can create shared variables and arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This implies the following, which is particularly true for multiprocessing and similar...\n",
    "\n",
    "- Due to administration let the processes WORK (not for a second but for longer time; experience will teach you).\n",
    "\n",
    "Here we focus on a simple facility in `multiprocessing` for convenient parallel execution of the SPMD or MPMD type - a \"pool\" of \"workers\".\n",
    "\n",
    "- Pool a worker pool, $W$, with $M$ workers is formed as $W = (W_0, \\ldots, W_{M−1})$. Normally as many as you have cores available (or physical threads). In principle we can have as many as we want but it is inefficient to have more workers than we have physical threads. The workers do the heavy lifting.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We form a number of tasks $N$ of which we should normally have $N \\gg M$: $T = (T_0,\\ldots,T_{N−1})$. If we have perfect control of the tasks and a priori knowledge we may sometimes be able to reduce $N$.\n",
    "- We submit the tasks to a scheduler $S$, which then feeds the tasks to the pool of workers. Often a round robin approach is used where the next task in line is served once we have a free worker available.\n",
    "- Possible to control the scheduling via asynchronous techniques if we wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Passing data *to* a process:\n",
    "- Large amounts of data can be read from a file (e.g. HDF5) or can be shared between processes.\n",
    "- Intermediate amounts of data can be shared, submitted via the process call or read from a file - whatever makes most sense.\n",
    "- Small amounts of data should normally always be passed from the master process to the worker process via \n",
    "the call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Passing data *from* a process:\n",
    "- Best not to let a process write to shared memory - we have locks but they are not fool-proof.\n",
    "- Large amounts of data can be stored in e.g. HDF5 files.\n",
    "- Intermediate amounts of data can be stored in files or returned by the process to the master process.\n",
    "- Small amounts of data should always be returned to the master process - may be collected and stored to a file later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`multiprocessing` pool of workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "M = mp.cpu_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pool = mp.Pool(processes=M)\n",
    "pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As simple as that...\n",
    "\n",
    "We have now created a pool of workers that we can run processes in parallel under."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SPMD-style processing\n",
    "\n",
    "Use the `multiprocessing.Pool.map_async` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Similar to the native Python `map` function - i.e. apply a given function to list of different data items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "def _f(d):\n",
    "    # Defines the f(d) function (f(d) is a single task)\n",
    "    time.sleep(float(d)/10.)\n",
    "    pid = os.getpid()\n",
    "    print(\" _f argument: {:2d}, process id: {:7d} \".format(d, pid))\n",
    "    return pid\n",
    "\n",
    "def _callback(dummy):\n",
    "    # Defines the callback function\n",
    "    print(\"Input to callback: {0}\".format(dummy))\n",
    "    print(\"Callback process id: {0}\".format(os.getpid()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definining and starting the actual parallel computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': # We have to use this to make it work in this interactive interpreter\n",
    "    print(\"Parent process id: {:7d}\".format(os.getpid()))\n",
    "    pool = mp.Pool(processes=M)\n",
    "    result = pool.map_async(_f, (30 ,15 ,2), callback=_callback)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(result.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Does it really execute in parallel?\n",
    "- Write a small script similar to the above example\n",
    "- Verify that the tasks actually do execute in parallel by:\n",
    "    - Writing a task function (`_f`) that makes the process sleep for a while\n",
    "    - Apply the function to a small tuple of times using: `map` and `multiprocessing.Pool.map_async` and measure how long the total execution time is in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some remarks\n",
    "\n",
    "- `map_async` is *asynchronous*. That is it detaches from the worker processes and allows the main script to continue (possibly doing other things) while the tasks execute.\n",
    "- The purpose of the callback is to signal when the processing is done.\n",
    "- Closing the pool (`pool.close()`) means no additional tasks can be added to it.\n",
    "- The call `pool.join()` means that the main process will halt here and wait until the workers are done.\n",
    "- Finally, `result.get()` fetches the results of the tasks. This will also have the effect of halting the main process to wait for the results if the tasks are not done yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Process status\n",
    "- `result.ready()`: If the result is ready `True` is returned and `False` if the result is not ready at the time when the command is issued.\n",
    "- `result.get()`: Wait until the result is ready and then return the result. This is an often used functionality when chained computations are needed - like when the result `result.get()` is used as input to another computation.\n",
    "- `result.wait()`: Requests a wait until the result is ready. This means that a sequence of commands like `result.wait()` followed by `result.ready()` always returns `True`.\n",
    "- `result.successful()`: If the result is available at the time the command is issued it does nothing, and if the result is not ready is raises an `AssertionError`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Chunk Size\n",
    "\n",
    "* `map_async` splits the workload iterable (list, tuple etc.) into a number of chunks.\n",
    "* Work is handed one chunk at a time to the workers.\n",
    "* A specific chunk size can be requested with the `chunksize` argument - the resulting chunk size will be *approximately* this size (appr. due to possible rounding).\n",
    "* Assume we have an integer amount $NL$ of data items we want to compute on $M$ workers.\n",
    "* A good rule of thumb over a large range of combinations $N$ vs. $L$ is `chunksize`$ = \\left\\lceil \\frac{N}{M} \\right\\rceil$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you do not need the asynchronous functionality, the results can be gathered directly (waiting for them) by using `multiprocessing.Pool.map()` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "def _f(d):\n",
    "    # Defines the f(d) function (f(d) is a single task)\n",
    "    time.sleep(float(d)/10.)\n",
    "    pid = os.getpid()\n",
    "    print(\" _f argument: {:2d}, process id: {:7d} \".format(d, pid))\n",
    "    return pid\n",
    "\n",
    "if __name__ == '__main__': # We have to use this to make it work in this interactive interpreter\n",
    "    print(\"Parent process id: {:7d}\".format(os.getpid()))\n",
    "    pool = mp.Pool(processes=M)\n",
    "    result = pool.map(_f, (30 ,15 ,2))\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MPMD-style processing\n",
    "\n",
    "Use the `multiprocessing.Pool.apply_async` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "def _f1(d):\n",
    "    # Defines the f1(d) function (f(d) is a single task)\n",
    "    return d + 1\n",
    "\n",
    "def _f2(d):\n",
    "    # Defines the f1(d) function (f(d) is a single task)c\n",
    "    return d + 2\n",
    "\n",
    "def _f3(d):\n",
    "    # Defines the f1(d) function (f(d) is a single task)c\n",
    "    return d + 3\n",
    "\n",
    "def _callback(dummy):\n",
    "    # Defines the callback function\n",
    "    print(\"Input to callback: {0}\".format(dummy))\n",
    "    print(\"Callback process id: {0}\".format(os.getpid()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': # We have to use this to make it work in this interactive interpreter\n",
    "    print(\"Parent process id: {:7d}\".format(os.getpid()))\n",
    "    pool = mp.Pool(processes=M)\n",
    "    results = []\n",
    "    results.append(pool.apply_async(_f1, (1,), callback=_callback))\n",
    "    results.append(pool.apply_async(_f2, (1,), callback=_callback))\n",
    "    results.append(pool.apply_async(_f3, (1,), callback=_callback))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print([result.get() for result in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Again, does it really execute in parallel?\n",
    "- Write a small script similar to the first exercise\n",
    "- Verify that the tasks actually do execute in parallel by:\n",
    "    - Writing task functions (`_f1` etc.) that make the process sleep for a while\n",
    "    - Apply each of the functions to a tuple containing one time value each by: calling the functions directly one after another and by using `multiprocessing.Pool.apply_async` and measure how long the total execution time is in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shared memory\n",
    "\n",
    "- Allows us to save memory as sharing is allowed between tasks.\n",
    "- Can (in principle) be used for both read and write...\n",
    "- Recommendation: Although a locking mechanism exists (see [multiprocessing.Lock](https://docs.python.org/3.7/library/multiprocessing.html#multiprocessing.Lock)) it is not foolproof - it only ensures that multiple tasks cannot concurrently write to the same memory.\n",
    "- ...but what happens if one worker writes to the same data before another and you didn’t expect that order of writing?\n",
    "- When using shared memory for reading set `lock=False` - otherwise you cannot have concurrent reads which is the point of it all.\n",
    "- If you use shared arrays for read-only then remember a unit test to ensure data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alternatives:\n",
    "- Transfer variables via function calls where different types of data can be packed.\n",
    "- For large amounts of data it is often preferred with selected fetching from e.g. an HDF5 file that allows parallel access.\n",
    "- Sharing via dedicated communication channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can share data across processes with the `multiprocessing.sharedctypes` module:  \n",
    "*(NB: more convenient mechanism `multiprocessing.shared_memory` under way in [Python 3.8](https://docs.python.org/3/library/multiprocessing.shared_memory.html))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import sharedctypes\n",
    "import numpy as np\n",
    "\n",
    "def task_func(args):\n",
    "    position, block_size = args\n",
    "    np_array = np.ctypeslib.as_array(array_shared_by_processes)\n",
    "    return np_array[position:position+block_size]\n",
    "\n",
    "def _init(the_array):\n",
    "    global array_shared_by_processes\n",
    "    array_shared_by_processes = the_array\n",
    "\n",
    "size = 12\n",
    "blocksize = 3\n",
    "shared_array = sharedctypes.RawArray(sharedctypes.ctypes.c_double, size)\n",
    "np.ctypeslib.as_array(shared_array)[:] = np.arange(size)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = mp.Pool(processes=4, initializer=_init, initargs=(shared_array, ))\n",
    "    result = pool.map(task_func, list(zip(range(0, size, blocksize), [blocksize]*int(size/blocksize))))\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "*Above example inspired by: http://thousandfold.net/cz/2014/05/01/sharing-numpy-arrays-between-processes-using-multiprocessing-and-ctypes/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remarks:\n",
    "- We create a `sharedctypes.RawArray` for the shared data.\n",
    "- This can be interfaced as a NumPy array using `np.ctypeslib.as_array`.\n",
    "- We use an initialiser function to declare this array as a global variable so that each of the task function processes can see it.\n",
    "- In the task function the array is interfaced as a NumPy array again.\n",
    "- The global variable is a trick we use, because the `shared_array` variable cannot easily be passed by `pool.map` directly.\n",
    "- We *can* also write to the shared array, but this is what we do not recommend here for safety reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- `multiprocessing` provides a fairly straightforward mechanism to explicitly set up parallel computations of the SPMD or MPMD type.\n",
    "- Due to the internal workings of Python (GIL), true parallelism is a bit \"heavy\" (separate processes each with their own copy of interpreter, variables etc.).\n",
    "- Threads also possible (we have not shown how here), but only useful for I/O-heavy concurrency with a lot of waiting (not true parallelism)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## concurrent.futures\n",
    "This is a new module that comes with Python 3 and has some of the same facilities as `multiprocessing`.\n",
    "- `concurrent.futures.ProcessPoolExecutor` has more or less corresponding functionality to `multiprocessing.Pool`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Advantages:\n",
    "- If your tasks can happen to crash beyond Python's control, `multiprocessing.Pool` can hang indefinitely. You will not notice until running your tasks takes suspiciously long. This should not happen using `concurrent.futures.ProcessPoolExecutor` (https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor)\n",
    "- `concurrent.futures` has the same interface to threads and processes, so switching between threads and processes in your code is simple (not relevant for simultanous parallel processing, but possibly for I/O-bound tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Speculation:\n",
    "- `concurrent.futures.ProcessPoolExecutor` might eventually replace `multiprocessing.Pool` down the line, so it could be practical to know about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SPMD-style processing\n",
    "\n",
    "Use `concurrent.futures.ProcessPoolExecutor.map`\n",
    "- Similar to `multiprocessing.Pool.map_async` - i.e. apply a given function to iterable of different data items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent import futures\n",
    "import os, time\n",
    "\n",
    "def _f(d):\n",
    "    # Defines the f(d) function (f(d) is a single task)\n",
    "    time.sleep(float(d)/10.)\n",
    "    pid = os.getpid()\n",
    "    print(\" _f argument: {:2d}, process id: {:7d} \".format(d, pid))\n",
    "    return pid\n",
    "\n",
    "if __name__ == '__main__': # We have to use this to make it work in this interactive interpreter\n",
    "    print(\"Parent process id: {:7d}\".format(os.getpid()))\n",
    "    M = os.cpu_count()\n",
    "    pool = futures.ProcessPoolExecutor(max_workers=M)\n",
    "    future = pool.map(_f, (30 ,15 ,2))\n",
    "    print('Something funny')\n",
    "    pool.shutdown()\n",
    "    print(list(future))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MPMD-style processing\n",
    "\n",
    "Use `concurrent.futures.ProcessPoolExecutor.submit`\n",
    "\n",
    "Similar to `multiprocessing.Pool.apply_async`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def _f1(d):\n",
    "    # Defines the f1(d) function (f(d) is a single task)\n",
    "    return d + 1\n",
    "\n",
    "def _f2(d):\n",
    "    # Defines the f1(d) function (f(d) is a single task)\n",
    "    return d + 2\n",
    "\n",
    "def _f3(d):\n",
    "    # Defines the f1(d) function (f(d) is a single task)\n",
    "    return d + 3\n",
    "\n",
    "if __name__ == '__main__': # We have to use this to make it work in this interactive interpreter\n",
    "    pool = futures.ProcessPoolExecutor(max_workers=M)\n",
    "    futurelist = []\n",
    "    futurelist.append(pool.submit(_f1, 1))\n",
    "    futurelist.append(pool.submit(_f2, 1))\n",
    "    futurelist.append(pool.submit(_f3, 1))\n",
    "    pool.shutdown()\n",
    "    print([future.result() for future in futurelist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- `concurrent.futures` basically provides the same parallel computing facilities as `multiprocessing` as far as worker pools are concerned.\n",
    "- `concurrent.futures` uses `multiprocessing` \"under the hood\", but has its own implementation of pools that in particular behaves better if your tasks can crash beyond the control of `multiprocessing.Pool` (cause segmentation fault).\n",
    "- Similar interfaces for threads and processes makes it very easy to switch between the two. Remember that threads are still not truly parallel - like in `multiprocessing`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "`concurrent.futures`\n",
    "- Write a small script implementing MPMD-style processing.\n",
    "- Define two functions to submit as tasks:\n",
    "    - `f1` adds a number to its input and returns the result.\n",
    "    - `f2` multiplies its input by a number and returns the result.\n",
    "- Apply the `f1` function to some scalar value as the first task.\n",
    "- Apply as second task the `f2` function to the future returned by the first task.\n",
    "- What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is Dask?\n",
    "\n",
    "A flexible parallel computing library by Continuum Analytics (the company behind Anaconda).\n",
    "- Provides several facilities that mimic existing popular Python packages for data analytics and numerical computing:\n",
    "    - Mimics `array`s from NumPy.\n",
    "    - Mimics `DataFrame`s from Pandas.\n",
    "    - ...as well as other features...\n",
    "- Here we focus on `array`s and the ability to parallellise tasks using the convenient `delayed` feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dask arrays\n",
    "\n",
    "Dask arrays mimic NumPy arrays, i.e. they behave like NumPy arrays but only provide a subset of the functionality of NumPy arrays.\n",
    "- Particularly good for large amounts of data that do not fit in memory.\n",
    "- Data can reside on disk transparently and is conveniently loaded as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Creating Dask arrays\n",
    "\n",
    "We can create Dask arrays from NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from dask import array as da\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(1000,1000)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dx = da.from_array(x, chunks=x.shape)\n",
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "type(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Note that `dx` is not a NumPy array - it is a Dask array; behaves similarly but is not the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We cannot directly see the contents of `dx`\n",
    "- They have not been computed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dx[:5,:5].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Computing with Dask arrays\n",
    "\n",
    "We can now manipulate the Dask array in NumPy-like ways.\n",
    "\n",
    "**Example - matrix-vector product**\n",
    "\n",
    "Create a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dy = da.random.random((1000,1), chunks=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Form the product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dprod = dx.dot(dy)\n",
    "dprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dprod[:10,0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Chunk size matters\n",
    "\n",
    "Notice the `chunks` argument to the Dask array creation functions. Like we saw with `multiprocessing.map_async`, the chunk size has a significant impact on the performance of our computations.\n",
    "\n",
    "From the [Dask documentation](https://docs.dask.org/en/latest/array-chunks.html)\n",
    "- A chunk should be small enough to fit comfortably in memory. We’ll have many chunks in memory at once.\n",
    "- Chunks must be large enough so that computations on them take $\\ll$ 1ms overhead per task that Dask scheduling incurs. A task should take longer than 100ms.\n",
    "- Chunks should align with the computation that you want to do. For example if you plan to frequently slice along a particular dimension then it is more efficient with chunks aligned so that you access fewer chunks. If you want to add two arrays then its convenient if those arrays have matching chunk patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example with different chunk sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dX = da.random.random((1000,1000),chunks=(10,100))\n",
    "dy = da.random.random((1000,1),chunks=(100,1))\n",
    "%timeit dX.dot(dy).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dX = da.random.random((1000,1000),chunks=(1000,100))\n",
    "%timeit dX.dot(dy).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Storing Dask arrays\n",
    "\n",
    "Dask arrays really are ideally suited for computations on large amounts of data that are too large to fit in memory.\n",
    "\n",
    "Therefore, it includes functionality to easily store Dask arrays to disk (if they are not already - above we created our dask arrays in memory): the function `to_hdf5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "da.to_hdf5('demofile.hdf5', '/X', dX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ls -lh demofile.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "HDF5 files are a popular file format for large data in scientific computing settings.\n",
    "- Very convenient to work with in several Python packages in data analysis, numerical computing etc. - for example Pandas, PyTables.\n",
    "- Good choice of file format for interoperability with other applications.\n",
    "- Also very convenient to work with in Dask.\n",
    "\n",
    "Dask can also use other file formats for storing data.\n",
    "- In fact it can use any file format that provides a NumPy slicing-like interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loading Dask arrays from disk\n",
    "\n",
    "This is what really justifies the use of Dask. The ability to handle large arrays of data on disk by automatically only loading into memory what is necessary.\n",
    "- Dask creates arrays based on the contents of files\n",
    "- The Dask array sits as an automatic and convenient NumPy array-like interface on top of the file\n",
    "- When performing operations on the arrays, Dask automatically loads the necessary data into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We load the previously stored 'demofile.hdf5' into a new Dask array.\n",
    "\n",
    "This is done the same way as we created an array based on a NumPy array before. Now we need to open the HDF5 file first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('demofile.hdf5','r+')\n",
    "dataset = f['X/']\n",
    "dY = da.from_array(dataset, chunks=dataset.shape)\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "da.all(dX == dY).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "da.all(dX == dY).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Dask arrays:\n",
    "- Write a small script that creates a new dask array.\n",
    "- Fill the Dask array with random values using a function from `dask.array.random`.\n",
    "- Store the array to an HDF5 file.\n",
    "- Repeat the above for different chunk sizes and time how long it takes to generate and store in each case. Does it make any difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Building parallel computations using `delayed`\n",
    "\n",
    "- So far we have looked at Dask arrays for NumPy-like operations.\n",
    "- Dask also has functionality for setting up parallel computations a bit like `multiprocessing.Pool.apply_async`.\n",
    "- Use the `dask.delayed.delayed` function:\n",
    "    - Wraps other functions and objects to be evaluated in parallel\n",
    "    - Can also be used as a decorator (not show here)\n",
    "- The terminology is conceptually similar to `concurrent.futures` where computations return \"future\" objects, the results of which are eventually computed using `.result()`. In Dask, computations return \"delayed\" objects whose results are eventually computed using `.compute()`.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example - sum all elements in an array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from dask.delayed import delayed\n",
    "import time\n",
    "\n",
    "def func(arr):\n",
    "    return arr.sum()\n",
    "\n",
    "t1 = time.time()\n",
    "# This part only sets up the computations but does not do anything yet\n",
    "darray = delayed(h5py.File('demofile.hdf5','r+')['/X'])\n",
    "sums = []\n",
    "for row in range(1000):\n",
    "    sums.append(delayed(func)(darray[row, :]))\n",
    "result = delayed(sum)(sums)\n",
    "\n",
    "t2 = time.time()\n",
    "# Here the actual computations are done    \n",
    "tmp = result.compute()\n",
    "\n",
    "t3 = time.time()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "t2 - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "t3 - t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- Dask effectively hides the setting up of parallel computations for our convenience.\n",
    "- Not a framework designed for explicitly controlling the individual tasks, processes etc.\n",
    "- Particularly suitable for computations on large data that does not fit into memory.\n",
    "- NB: the examples show here were all toy examples of a size where using Dask probably does not demonstrate its true strength and using NumPy etc. directly might easily be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other possibilities\n",
    "\n",
    "- Check out Joblib: https://joblib.readthedocs.io\n",
    "\n",
    "    - Easy interface to setting up parallel computations (uses `multiprocessing` behind the scenes).\n",
    "    - Caching of results to disk\n",
    "- Numerous possibilities listed here: https://wiki.python.org/moin/ParallelProcessing (the wiki may be somewhat out of date)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
